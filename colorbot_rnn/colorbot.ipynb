{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental notebook for colorbot example using eager mode with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.7.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.eager.python import tfe\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "\n",
    "data_dir = './colordata'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "  \"\"\"Parse a line from the colors dataset.\"\"\"\n",
    "\n",
    "  # Each line of the dataset is comma-separated and formatted as\n",
    "  #    color_name, r, g, b\n",
    "  # so `items` is a list [color_name, r, g, b].\n",
    "  items = tf.string_split([line], \",\").values\n",
    "  rgb = tf.string_to_number(items[1:], out_type=tf.float32) / 255.\n",
    "  # Represent the color name as a one-hot encoded character sequence.\n",
    "  color_name = items[0]\n",
    "  chars = tf.one_hot(tf.decode_raw(color_name, tf.uint8), depth=256)\n",
    "  # The sequence length is needed by our RNN.\n",
    "  length = tf.cast(tf.shape(chars)[0], dtype=tf.int64)\n",
    "  return rgb, chars, length\n",
    "\n",
    "\n",
    "def load_dataset(data_dir, url, batch_size):\n",
    "  \"\"\"Loads the colors data at path into a PaddedDataset.\"\"\"\n",
    "\n",
    "  # Downloads data at url into data_dir/basename(url). The dataset has a header\n",
    "  # row (color_name, r, g, b) followed by comma-separated lines.\n",
    "  path = tf.keras.utils.get_file(url.split('/')[-1], url, cache_subdir=data_dir)\n",
    "\n",
    "  # This chain of commands loads our data by:\n",
    "  #   1. skipping the header; (.skip(1))\n",
    "  #   2. parsing the subsequent lines; (.map(parse))\n",
    "  #   3. shuffling the data; (.shuffle(...))\n",
    "  #   3. grouping the data into padded batches (.padded_batch(...)).\n",
    "  dataset = tf.data.TextLineDataset(path).skip(1).map(parse_line).shuffle(\n",
    "      buffer_size=10000).padded_batch(\n",
    "          batch_size, padded_shapes=([None], [None, None], []))\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_TRAIN_URL = \"https://raw.githubusercontent.com/random-forests/tensorflow-workshop/master/extras/colorbot/data/train.csv\"\n",
    "SOURCE_TEST_URL = \"https://raw.githubusercontent.com/random-forests/tensorflow-workshop/master/extras/colorbot/data/test.csv\"\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "data_dir = os.path.join(data_dir, \"data\")\n",
    "data_dir = os.path.abspath(data_dir)\n",
    "train_data = load_dataset(\n",
    "  data_dir=data_dir, url=SOURCE_TRAIN_URL, batch_size=batch_size)\n",
    "eval_data = load_dataset(\n",
    "  data_dir=data_dir, url=SOURCE_TEST_URL, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNColorbot(tf.keras.Model):\n",
    "    \"\"\"Multi-layer (LSTM) RNN that regresses on real-valued vector labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rnn_cell_sizes, label_dimension, keep_prob):\n",
    "        \"\"\"Constructs an RNNColorbot.\n",
    "\n",
    "        Args:\n",
    "          rnn_cell_sizes: list of integers denoting the size of each LSTM cell in\n",
    "            the RNN; rnn_cell_sizes[i] is the size of the i-th layer cell\n",
    "          label_dimension: the length of the labels on which to regress\n",
    "          keep_prob: (1 - dropout probability); dropout is applied to the outputs of\n",
    "            each LSTM layer\n",
    "        \"\"\"\n",
    "        super(RNNColorbot, self).__init__(name=\"\")\n",
    "        self.label_dimension = label_dimension\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "        self.cells = self._add_cells(\n",
    "            [tf.nn.rnn_cell.BasicLSTMCell(size) for size in rnn_cell_sizes])\n",
    "        self.relu = tf.layers.Dense(\n",
    "            label_dimension, activation=tf.nn.relu, name=\"relu\")\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Implements the RNN logic and prediction generation.\n",
    "\n",
    "        Args:\n",
    "          inputs: A tuple (chars, sequence_length), where chars is a batch of\n",
    "            one-hot encoded color names represented as a Tensor with dimensions\n",
    "            [batch_size, time_steps, 256] and sequence_length holds the length\n",
    "            of each character sequence (color name) as a Tensor with dimension\n",
    "            [batch_size].\n",
    "          training: whether the invocation is happening during training\n",
    "\n",
    "        Returns:\n",
    "          A tensor of dimension [batch_size, label_dimension] that is produced by\n",
    "          passing chars through a multi-layer RNN and applying a ReLU to the final\n",
    "          hidden state.\n",
    "        \"\"\"\n",
    "        (chars, sequence_length) = inputs\n",
    "        # Transpose the first and second dimensions so that chars is of shape\n",
    "        # [time_steps, batch_size, dimension].\n",
    "        chars = tf.transpose(chars, [1, 0, 2])\n",
    "        # The outer loop cycles through the layers of the RNN; the inner loop\n",
    "        # executes the time steps for a particular layer.\n",
    "        batch_size = int(chars.shape[1])\n",
    "        for l in range(len(self.cells)):\n",
    "            cell = self.cells[l]\n",
    "            outputs = []\n",
    "            state = cell.zero_state(batch_size, tf.float32)\n",
    "            # Unstack the inputs to obtain a list of batches, one for each time step.\n",
    "            chars = tf.unstack(chars, axis=0)\n",
    "            for ch in chars:\n",
    "                output, state = cell(ch, state)\n",
    "                outputs.append(output)\n",
    "            # The outputs of this layer are the inputs of the subsequent layer.\n",
    "            chars = tf.stack(outputs, axis=0)\n",
    "            if training:\n",
    "                chars = tf.nn.dropout(chars, self.keep_prob)\n",
    "        # Extract the correct output (i.e., hidden state) for each example. All the\n",
    "        # character sequences in this batch were padded to the same fixed length so\n",
    "        # that they could be easily fed through the above RNN loop. The\n",
    "        # `sequence_length` vector tells us the true lengths of the character\n",
    "        # sequences, letting us obtain for each sequence the hidden state that was\n",
    "        # generated by its non-padding characters.\n",
    "        batch_range = [i for i in range(batch_size)]\n",
    "        indices = tf.stack([sequence_length - 1, batch_range], axis=1)\n",
    "        hidden_states = tf.gather_nd(chars, indices)\n",
    "        return self.relu(hidden_states)\n",
    "\n",
    "    \n",
    "    def _add_cells(self, cells):\n",
    "        # \"Magic\" required for keras.Model classes to track all the variables in\n",
    "        # a list of tf.layers.Layer objects.\n",
    "        # TODO(ashankar): Figure out API so user code doesn't have to do this.\n",
    "        for i, c in enumerate(cells):\n",
    "            setattr(self, \"cell-%d\" % i, c)\n",
    "        return cells\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_data, log_interval=10):\n",
    "    \"\"\"Trains model on train_data using optimizer.\"\"\"\n",
    "\n",
    "    tf.train.get_or_create_global_step()\n",
    "\n",
    "    def model_loss(labels, chars, sequence_length):\n",
    "        predictions = model((chars, sequence_length), training=True)\n",
    "        loss_value = loss(labels, predictions)\n",
    "        tf.contrib.summary.scalar(\"loss\", loss_value)\n",
    "        return loss_value\n",
    "\n",
    "    for (batch, (labels, chars, sequence_length)) in enumerate(\n",
    "        tfe.Iterator(train_data)):\n",
    "        with tf.contrib.summary.record_summaries_every_n_global_steps(log_interval):\n",
    "            batch_model_loss = functools.partial(model_loss, labels, chars,\n",
    "                                               sequence_length)\n",
    "            optimizer.minimize(\n",
    "              batch_model_loss, global_step=tf.train.get_global_step())\n",
    "            if log_interval and batch % log_interval == 0:\n",
    "                print(\"train/batch #%d\\tloss: %.6f\" % (batch, batch_model_loss()))\n",
    "\n",
    "\n",
    "def loss(labels, predictions):\n",
    "    \"\"\"Computes mean squared loss.\"\"\"\n",
    "    return tf.reduce_mean(tf.square(predictions - labels))\n",
    "\n",
    "\n",
    "def test(model, eval_data):\n",
    "    \"\"\"Computes the average loss on eval_data, which should be a Dataset.\"\"\"\n",
    "    avg_loss = tfe.metrics.Mean(\"loss\")\n",
    "    for (labels, chars, sequence_length) in tfe.Iterator(eval_data):\n",
    "        predictions = model((chars, sequence_length), training=False)\n",
    "        avg_loss(loss(labels, predictions))\n",
    "    print(\"eval/loss: %.6f\\n\" % avg_loss.result())\n",
    "    with tf.contrib.summary.always_record_summaries():\n",
    "        tf.contrib.summary.scalar(\"loss\", avg_loss.result())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device /cpu:0.\n",
      "train/batch #0\tloss: 0.195222\n",
      "train/batch #10\tloss: 0.139604\n",
      "train/time for epoch #0: 3.53\n",
      "eval/loss: 0.096366\n",
      "\n",
      "train/batch #0\tloss: 0.103895\n",
      "train/batch #10\tloss: 0.102696\n",
      "train/time for epoch #1: 3.38\n",
      "eval/loss: 0.087820\n",
      "\n",
      "train/batch #0\tloss: 0.086385\n",
      "train/batch #10\tloss: 0.082124\n",
      "train/time for epoch #2: 3.42\n",
      "eval/loss: 0.071092\n",
      "\n",
      "train/batch #0\tloss: 0.078862\n",
      "train/batch #10\tloss: 0.079547\n",
      "train/time for epoch #3: 3.39\n",
      "eval/loss: 0.068192\n",
      "\n",
      "train/batch #0\tloss: 0.075266\n",
      "train/batch #10\tloss: 0.069411\n",
      "train/time for epoch #4: 3.43\n",
      "eval/loss: 0.069553\n",
      "\n",
      "train/batch #0\tloss: 0.083395\n",
      "train/batch #10\tloss: 0.060095\n",
      "train/time for epoch #5: 3.26\n",
      "eval/loss: 0.068354\n",
      "\n",
      "train/batch #0\tloss: 0.069051\n",
      "train/batch #10\tloss: 0.062178\n",
      "train/time for epoch #6: 3.38\n",
      "eval/loss: 0.066311\n",
      "\n",
      "train/batch #0\tloss: 0.081664\n",
      "train/batch #10\tloss: 0.081469\n",
      "train/time for epoch #7: 3.35\n",
      "eval/loss: 0.063767\n",
      "\n",
      "train/batch #0\tloss: 0.071368\n",
      "train/batch #10\tloss: 0.070541\n",
      "train/time for epoch #8: 3.45\n",
      "eval/loss: 0.063108\n",
      "\n",
      "train/batch #0\tloss: 0.068747\n",
      "train/batch #10\tloss: 0.070923\n",
      "train/time for epoch #9: 3.37\n",
      "eval/loss: 0.064460\n",
      "\n",
      "train/batch #0\tloss: 0.073688\n",
      "train/batch #10\tloss: 0.075936\n",
      "train/time for epoch #10: 3.45\n",
      "eval/loss: 0.067205\n",
      "\n",
      "train/batch #0\tloss: 0.052262\n",
      "train/batch #10\tloss: 0.068889\n",
      "train/time for epoch #11: 3.46\n",
      "eval/loss: 0.070621\n",
      "\n",
      "train/batch #0\tloss: 0.071453\n",
      "train/batch #10\tloss: 0.062407\n",
      "train/time for epoch #12: 3.38\n",
      "eval/loss: 0.063960\n",
      "\n",
      "train/batch #0\tloss: 0.060531\n",
      "train/batch #10\tloss: 0.051849\n",
      "train/time for epoch #13: 3.34\n",
      "eval/loss: 0.065506\n",
      "\n",
      "train/batch #0\tloss: 0.054709\n",
      "train/batch #10\tloss: 0.068768\n",
      "train/time for epoch #14: 3.38\n",
      "eval/loss: 0.062028\n",
      "\n",
      "train/batch #0\tloss: 0.059481\n",
      "train/batch #10\tloss: 0.051233\n",
      "train/time for epoch #15: 3.45\n",
      "eval/loss: 0.063468\n",
      "\n",
      "train/batch #0\tloss: 0.053662\n",
      "train/batch #10\tloss: 0.051848\n",
      "train/time for epoch #16: 3.37\n",
      "eval/loss: 0.063615\n",
      "\n",
      "train/batch #0\tloss: 0.059366\n",
      "train/batch #10\tloss: 0.060566\n",
      "train/time for epoch #17: 3.40\n",
      "eval/loss: 0.057061\n",
      "\n",
      "train/batch #0\tloss: 0.047426\n",
      "train/batch #10\tloss: 0.057168\n",
      "train/time for epoch #18: 3.32\n",
      "eval/loss: 0.059871\n",
      "\n",
      "train/batch #0\tloss: 0.047698\n",
      "train/batch #10\tloss: 0.051760\n",
      "train/time for epoch #19: 3.45\n",
      "eval/loss: 0.065920\n",
      "\n",
      "train/batch #0\tloss: 0.061790\n",
      "train/batch #10\tloss: 0.051041\n",
      "train/time for epoch #20: 3.30\n",
      "eval/loss: 0.061253\n",
      "\n",
      "train/batch #0\tloss: 0.055514\n",
      "train/batch #10\tloss: 0.061015\n",
      "train/time for epoch #21: 3.30\n",
      "eval/loss: 0.064311\n",
      "\n",
      "train/batch #0\tloss: 0.058133\n",
      "train/batch #10\tloss: 0.044714\n",
      "train/time for epoch #22: 3.40\n",
      "eval/loss: 0.068682\n",
      "\n",
      "train/batch #0\tloss: 0.044063\n",
      "train/batch #10\tloss: 0.048973\n",
      "train/time for epoch #23: 3.39\n",
      "eval/loss: 0.060642\n",
      "\n",
      "train/batch #0\tloss: 0.054484\n",
      "train/batch #10\tloss: 0.050576\n",
      "train/time for epoch #24: 3.39\n",
      "eval/loss: 0.066651\n",
      "\n",
      "train/batch #0\tloss: 0.041337\n",
      "train/batch #10\tloss: 0.043867\n",
      "train/time for epoch #25: 3.33\n",
      "eval/loss: 0.055767\n",
      "\n",
      "train/batch #0\tloss: 0.044576\n",
      "train/batch #10\tloss: 0.053707\n",
      "train/time for epoch #26: 3.62\n",
      "eval/loss: 0.063808\n",
      "\n",
      "train/batch #0\tloss: 0.051076\n",
      "train/batch #10\tloss: 0.046988\n",
      "train/time for epoch #27: 3.71\n",
      "eval/loss: 0.058284\n",
      "\n",
      "train/batch #0\tloss: 0.040092\n",
      "train/batch #10\tloss: 0.039958\n",
      "train/time for epoch #28: 3.37\n",
      "eval/loss: 0.069695\n",
      "\n",
      "train/batch #0\tloss: 0.046855\n",
      "train/batch #10\tloss: 0.046808\n",
      "train/time for epoch #29: 3.42\n",
      "eval/loss: 0.064688\n",
      "\n",
      "train/batch #0\tloss: 0.046407\n",
      "train/batch #10\tloss: 0.040891\n",
      "train/time for epoch #30: 3.51\n",
      "eval/loss: 0.068231\n",
      "\n",
      "train/batch #0\tloss: 0.034400\n",
      "train/batch #10\tloss: 0.042645\n",
      "train/time for epoch #31: 3.43\n",
      "eval/loss: 0.080662\n",
      "\n",
      "train/batch #0\tloss: 0.029798\n",
      "train/batch #10\tloss: 0.038068\n",
      "train/time for epoch #32: 3.38\n",
      "eval/loss: 0.064323\n",
      "\n",
      "train/batch #0\tloss: 0.033454\n",
      "train/batch #10\tloss: 0.035057\n",
      "train/time for epoch #33: 3.40\n",
      "eval/loss: 0.062405\n",
      "\n",
      "train/batch #0\tloss: 0.039604\n",
      "train/batch #10\tloss: 0.031694\n",
      "train/time for epoch #34: 3.35\n",
      "eval/loss: 0.066001\n",
      "\n",
      "train/batch #0\tloss: 0.024022\n",
      "train/batch #10\tloss: 0.028815\n",
      "train/time for epoch #35: 3.45\n",
      "eval/loss: 0.067008\n",
      "\n",
      "train/batch #0\tloss: 0.025555\n",
      "train/batch #10\tloss: 0.029216\n",
      "train/time for epoch #36: 3.43\n",
      "eval/loss: 0.074540\n",
      "\n",
      "train/batch #0\tloss: 0.035828\n",
      "train/batch #10\tloss: 0.032771\n",
      "train/time for epoch #37: 3.47\n",
      "eval/loss: 0.072987\n",
      "\n",
      "train/batch #0\tloss: 0.027119\n",
      "train/batch #10\tloss: 0.032110\n",
      "train/time for epoch #38: 3.39\n",
      "eval/loss: 0.069661\n",
      "\n",
      "train/batch #0\tloss: 0.023551\n",
      "train/batch #10\tloss: 0.027573\n",
      "train/time for epoch #39: 3.38\n",
      "eval/loss: 0.071210\n",
      "\n",
      "train/batch #0\tloss: 0.019065\n",
      "train/batch #10\tloss: 0.030441\n",
      "train/time for epoch #40: 3.43\n",
      "eval/loss: 0.067529\n",
      "\n",
      "train/batch #0\tloss: 0.025389\n",
      "train/batch #10\tloss: 0.031370\n",
      "train/time for epoch #41: 3.45\n",
      "eval/loss: 0.071808\n",
      "\n",
      "train/batch #0\tloss: 0.020649\n",
      "train/batch #10\tloss: 0.027096\n",
      "train/time for epoch #42: 3.34\n",
      "eval/loss: 0.077113\n",
      "\n",
      "train/batch #0\tloss: 0.025226\n",
      "train/batch #10\tloss: 0.022226\n",
      "train/time for epoch #43: 3.40\n",
      "eval/loss: 0.069148\n",
      "\n",
      "train/batch #0\tloss: 0.020658\n",
      "train/batch #10\tloss: 0.021723\n",
      "train/time for epoch #44: 3.43\n",
      "eval/loss: 0.072138\n",
      "\n",
      "train/batch #0\tloss: 0.022201\n",
      "train/batch #10\tloss: 0.019682\n",
      "train/time for epoch #45: 3.33\n",
      "eval/loss: 0.074668\n",
      "\n",
      "train/batch #0\tloss: 0.018337\n",
      "train/batch #10\tloss: 0.019651\n",
      "train/time for epoch #46: 3.36\n",
      "eval/loss: 0.071319\n",
      "\n",
      "train/batch #0\tloss: 0.019682\n",
      "train/batch #10\tloss: 0.018935\n",
      "train/time for epoch #47: 3.47\n",
      "eval/loss: 0.078457\n",
      "\n",
      "train/batch #0\tloss: 0.021202\n",
      "train/batch #10\tloss: 0.030652\n",
      "train/time for epoch #48: 3.39\n",
      "eval/loss: 0.075587\n",
      "\n",
      "train/batch #0\tloss: 0.018431\n",
      "train/batch #10\tloss: 0.020425\n",
      "train/time for epoch #49: 3.30\n",
      "eval/loss: 0.071251\n",
      "\n",
      "train/batch #0\tloss: 0.021232\n",
      "train/batch #10\tloss: 0.018206\n",
      "train/time for epoch #50: 3.37\n",
      "eval/loss: 0.072947\n",
      "\n",
      "train/batch #0\tloss: 0.015040\n",
      "train/batch #10\tloss: 0.020530\n",
      "train/time for epoch #51: 3.35\n",
      "eval/loss: 0.074494\n",
      "\n",
      "train/batch #0\tloss: 0.015123\n",
      "train/batch #10\tloss: 0.015893\n",
      "train/time for epoch #52: 3.42\n",
      "eval/loss: 0.068059\n",
      "\n",
      "train/batch #0\tloss: 0.014066\n",
      "train/batch #10\tloss: 0.015930\n",
      "train/time for epoch #53: 3.33\n",
      "eval/loss: 0.074426\n",
      "\n",
      "train/batch #0\tloss: 0.015896\n",
      "train/batch #10\tloss: 0.014845\n",
      "train/time for epoch #54: 3.35\n",
      "eval/loss: 0.070767\n",
      "\n",
      "train/batch #0\tloss: 0.016820\n",
      "train/batch #10\tloss: 0.012725\n",
      "train/time for epoch #55: 3.47\n",
      "eval/loss: 0.074426\n",
      "\n",
      "train/batch #0\tloss: 0.017321\n",
      "train/batch #10\tloss: 0.018120\n",
      "train/time for epoch #56: 3.33\n",
      "eval/loss: 0.075348\n",
      "\n",
      "train/batch #0\tloss: 0.011252\n",
      "train/batch #10\tloss: 0.015438\n",
      "train/time for epoch #57: 3.41\n",
      "eval/loss: 0.072515\n",
      "\n",
      "train/batch #0\tloss: 0.015094\n",
      "train/batch #10\tloss: 0.015802\n",
      "train/time for epoch #58: 3.44\n",
      "eval/loss: 0.071852\n",
      "\n",
      "train/batch #0\tloss: 0.011733\n",
      "train/batch #10\tloss: 0.015191\n",
      "train/time for epoch #59: 3.40\n",
      "eval/loss: 0.076679\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# useful params.\n",
    "\n",
    "use_gpu = False\n",
    "num_epochs = 60\n",
    "log_interval = 10\n",
    "\n",
    "if use_gpu and tfe.num_gpus() > 0:\n",
    "    device = \"/gpu:0\"\n",
    "    print(tfe.num_gpus())\n",
    "else:\n",
    "    device = \"/cpu:0\"\n",
    "print(\"Using device %s.\" % device)\n",
    "\n",
    "log_dir = os.path.join(data_dir, \"summaries\")\n",
    "tf.gfile.MakeDirs(log_dir)\n",
    "\n",
    "train_summary_writer = tf.contrib.summary.create_file_writer(\n",
    "  os.path.join(log_dir, \"train\"), flush_millis=10000)\n",
    "test_summary_writer = tf.contrib.summary.create_file_writer(\n",
    "  os.path.join(log_dir, \"eval\"), flush_millis=10000, name=\"eval\")\n",
    "\n",
    "\n",
    "model = RNNColorbot(\n",
    "  rnn_cell_sizes=[256, 128],\n",
    "  label_dimension=3,\n",
    "  keep_prob=0.5)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "\n",
    "\n",
    "with tf.device(device):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        with train_summary_writer.as_default():\n",
    "            train_one_epoch(model, optimizer, train_data, log_interval)\n",
    "        end = time.time()\n",
    "        print(\"train/time for epoch #%d: %.2f\" % (epoch, end - start))\n",
    "        with test_summary_writer.as_default():\n",
    "            test(model, eval_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colorbot is ready to generate colors!\n"
     ]
    }
   ],
   "source": [
    "print(\"Colorbot is ready to generate colors!\")\n",
    "# while True:\n",
    "#     try:\n",
    "#         color_name = six.moves.input(\n",
    "#           \"Give me a color name (or press enter to exit): \")\n",
    "#     except EOFError:\n",
    "#         return\n",
    "\n",
    "def check_color(color_name):\n",
    "    _, chars, length = parse_line(color_name)\n",
    "    with tf.device(device):\n",
    "        (chars, length) = (tf.identity(chars), tf.identity(length))\n",
    "        chars = tf.expand_dims(chars, 0)\n",
    "        length = tf.expand_dims(length, 0)\n",
    "        preds = tf.unstack(model((chars, length), training=False)[0])\n",
    "\n",
    "    # Predictions cannot be negative, as they are generated by a ReLU layer;\n",
    "    # they may, however, be greater than 1.\n",
    "    clipped_preds = tuple(min(float(p), 1.0) for p in preds)\n",
    "    print('predictions:', [float(x) for x in preds])\n",
    "    rgb = tuple(int(p * 255) for p in clipped_preds)\n",
    "    print(\"rgb:\", rgb)\n",
    "    data = [[clipped_preds]]\n",
    "    plt.imshow(data)\n",
    "    plt.title(color_name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0.05765679478645325, 0.2732844054698944, 0.8310462236404419]\n",
      "rgb: (14, 69, 211)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQoAAAEICAYAAACnA7rCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADU9JREFUeJzt3X+s3XV9x/Hni9ZKNkABDVTaAZvdH8UZ1DumfzhMqFnZ\nD2riNmEaIcF1C5q4qH806eIfqIlo/PEHJLNzzk6zgJJsNqFGodP4x4R5E5kJGGwlmhYRJhY3QxSY\n7/1xv92ON+fed7tz7rn30ucjae73+z2fns+bS/u855570pOqQpKWc8ZqDyBp7TMUklqGQlLLUEhq\nGQpJLUMhqWUotKQk30uyY8z11yU5thozaXUYCkktQyGpZSjU+e0kDyY5nuTvk5y5eEGSSvLSkfNP\nJ3n/yPkfJrk/yZNJ/jXJy2c1vKbDUKjzZuD3gN8AfhP461P5zUleAXwK+AvgfOATwIEkz5/ynFpB\nhkKdW6vqaFX9GPgAcN0p/v7dwCeq6r6q+u+q2g/8HHj1tAfVyjEU6hwdOf4+8JJT/P0XA+8evu14\nMsmTwNb/x/1oFW1c7QG05m0dOf414Adj1jwF/MrI+YXAiR+fHgU+UFUfWJnxNAs+olDn7Um2JDkP\n2AvcMWbN/cCfJdmQZCdw5chtfwv8ZZLfyYJfTfIHSc6eweyaEkOhzj8CXwYeBr4LvH/MmncCfwQ8\nycKTn/984oaqmgf+HLgVOA4cAW5Y0Yk1dfEfrpHU8RGFpJahkNQyFJJahkJSa82+jiKbzq0zzvQ1\nOdJK+sV/Pfijqnpxt27NhuKMM1/CWa8e9yN7SdPyn3f/1vdPZp3fekhqGQpJLUMhqWUoJLUMhaSW\noZDUMhSSWoZCUstQSGoZCkktQyGpZSgktQyFpJahkNQyFJJahkJSy1BIak0UiiTnJbk7yeHh47nL\nrD0nybEkt06yp6TZm/QRxR7gUFVtAw4N50t5H/C1CfeTtAomDcUuYP9wvB94w7hFSV4FXMDCW9NJ\nWmcmDcUFVfXocPxDFmLwS5KcAXwEeE93Z0l2J5lPMl/PHJ9wNEnT0v4r3EnuYeFt7BfbO3pSVZVk\n3BuZ3gQcrKpjSZbdq6r2AfsANpxzmW+KKq0RbSiqasdStyV5LMnmqno0yWbg8THLXgO8NslNwFnA\npiQ/rarlns+QtIZM+r4eB4DrgQ8OH7+weEFVvfnEcZIbgDkjIa0vkz5H8UHg9UkOAzuGc5LMJfnk\npMNJWhsmekRRVU8AV425Pg+8bcz1TwOfnmRPSbPnKzMltQyFpJahkNQyFJJahkJSy1BIahkKSS1D\nIallKCS1DIWklqGQ1DIUklqGQlLLUEhqGQpJLUMhqWUoJLUMhaSWoZDUMhSSWoZCUstQSGoZCkkt\nQyGpZSgktQyFpJahkNQyFJJahkJSy1BIahkKSS1DIallKCS1DIWk1kShSHJekruTHB4+njtmzeVJ\nvp7kgSTfSvKmSfaUNHuTPqLYAxyqqm3AoeF8saeAt1bVZcBO4ONJXjjhvpJmaNJQ7AL2D8f7gTcs\nXlBV36mqw8PxD4DHgRdPuK+kGZo0FBdU1aPD8Q+BC5ZbnOQKYBPw3Qn3lTRDG7sFSe4BLhxz097R\nk6qqJLXM/WwGPgNcX1W/WGLNbmA3QM7c3I0maUbaUFTVjqVuS/JYks1V9egQgseXWHcOcBewt6ru\nXWavfcA+gA3nXLZkdCTN1qTfehwArh+Orwe+sHhBkk3APwH/UFV3TrifpFUwaSg+CLw+yWFgx3BO\nkrkknxzW/Cnwu8ANSe4ffl0+4b6SZqj91mM5VfUEcNWY6/PA24bjzwKfnWQfSavLV2ZKahkKSS1D\nIallKCS1DIWklqGQ1DIUklqGQlLLUEhqGQpJLUMhqWUoJLUMhaSWoZDUMhSSWoZCUstQSGoZCkkt\nQyGpZSgktQyFpJahkNQyFJJahkJSy1BIahkKSS1DIallKCS1DIWklqGQ1DIUklqGQlLLUEhqGQpJ\nLUMhqTWVUCTZmeShJEeS7Blz+/OT3DHcfl+SS6axr6TZmDgUSTYAtwFXA9uB65JsX7TsRuB4Vb0U\n+Bhwy6T7SpqdaTyiuAI4UlUPV9XTwO3ArkVrdgH7h+M7gauSZAp7S5qBaYTiIuDoyPmx4drYNVX1\nLPAT4PzFd5Rkd5L5JPP1zPEpjCZpGtbUk5lVta+q5qpqLs87d7XHkTSYRigeAbaOnG8Zro1dk2Qj\n8ALgiSnsLWkGphGKbwDbklyaZBNwLXBg0ZoDwPXD8R8D/1JVNYW9Jc3AxknvoKqeTfIO4EvABuBT\nVfVAkpuB+ao6APwd8JkkR4AfsxATSevExKEAqKqDwMFF1947cvwz4E+msZek2VtTT2ZKWpsMhaSW\noZDUMhSSWoZCUstQSGoZCkktQyGpZSgktQyFpJahkNQyFJJahkJSy1BIahkKSS1DIallKCS1DIWk\nlqGQ1DIUklqGQlLLUEhqGQpJLUMhqWUoJLUMhaSWoZDUMhSSWoZCUstQSGoZCkktQyGpZSgktQyF\npNZUQpFkZ5KHkhxJsmfM7e9K8mCSbyU5lOTiaewraTYmDkWSDcBtwNXAduC6JNsXLfsmMFdVLwfu\nBD406b6SZmcajyiuAI5U1cNV9TRwO7BrdEFVfaWqnhpO7wW2TGFfSTMyjVBcBBwdOT82XFvKjcAX\np7CvpBnZOMvNkrwFmAOuXOL23cBugJy5eYaTSVrONB5RPAJsHTnfMlz7JUl2AHuBa6rq5+PuqKr2\nVdVcVc3leedOYTRJ0zCNUHwD2Jbk0iSbgGuBA6MLkrwC+AQLkXh8CntKmqGJQ1FVzwLvAL4EfBv4\nXFU9kOTmJNcMyz4MnAV8Psn9SQ4scXeS1qCpPEdRVQeBg4uuvXfkeMc09pG0OnxlpqSWoZDUMhSS\nWoZCUstQSGoZCkktQyGpZSgktQyFpJahkNQyFJJahkJSy1BIahkKSS1DIallKCS1DIWklqGQ1DIU\nklqGQlLLUEhqGQpJLUMhqWUoJLUMhaSWoZDUMhSSWoZCUstQSGoZCkktQyGpZSgktQyFpJahkNQy\nFJJaUwlFkp1JHkpyJMmeZda9MUklmZvGvpJmY+JQJNkA3AZcDWwHrkuyfcy6s4F3AvdNuqek2ZrG\nI4orgCNV9XBVPQ3cDuwas+59wC3Az6awp6QZmkYoLgKOjpwfG679rySvBLZW1V3L3VGS3Unmk8zX\nM8enMJqkaVjxJzOTnAF8FHh3t7aq9lXVXFXN5XnnrvRokk7SNELxCLB15HzLcO2Es4GXAV9N8j3g\n1cABn9CU1o9phOIbwLYklybZBFwLHDhxY1X9pKpeVFWXVNUlwL3ANVU1P4W9Jc3AxKGoqmeBdwBf\nAr4NfK6qHkhyc5JrJr1/Satv4zTupKoOAgcXXXvvEmtfN409Jc2Or8yU1DIUklqGQlLLUEhqGQpJ\nLUMhqWUoJLUMhaSWoZDUMhSSWoZCUstQSGoZCkktQyGpZSgktQyFpFaqarVnGCvJfwDfX4G7fhHw\noxW435WynuZdT7PC+pp3pWa9uKpe3C1as6FYKUnmq2rd/MO+62ne9TQrrK95V3tWv/WQ1DIUklqn\nYyj2rfYAp2g9zbueZoX1Ne+qznraPUch6dSdjo8oJJ0iQyGp9ZwPRZLzktyd5PDwccl3P05yTpJj\nSW6d5YyLZmjnTXJ5kq8neSDJt5K8acYz7kzyUJIjSfaMuf35Se4Ybr8vySWznG/RLN2s70ry4PB5\nPJTk4tWYc2SeZecdWffGJDWr9/B9zocC2AMcqqptwKHhfCnvA742k6mWdjLzPgW8taouA3YCH0/y\nwlkMl2QDcBtwNbAduC7J9kXLbgSOV9VLgY8Bt8xitsVOctZvAnNV9XLgTuBDs53y/5zkvCQ5G3gn\ncN+sZjsdQrEL2D8c7wfeMG5RklcBFwBfntFcS2nnrarvVNXh4fgHwONA++q6KbkCOFJVD1fV08Dt\nLMw8avS/4U7gqiSZ0Xyj2lmr6itV9dRwei+wZcYzjjqZzy0sfEG7BfjZrAY7HUJxQVU9Ohz/kIUY\n/JIkZwAfAd4zy8GW0M47KskVwCbguys92OAi4OjI+bHh2tg1w5tY/wQ4fybTLTHHYNyso24Evrii\nEy2vnTfJK4GtVXXXLAebypsUr7Yk9wAXjrlp7+hJVVWScT8Pvgk4WFXHZvGFbwrznrifzcBngOur\n6hfTnfL0kuQtwBxw5WrPspThC9pHgRtmvfdzIhRVtWOp25I8lmRzVT06/MV6fMyy1wCvTXITcBaw\nKclPq2q55zNWc16SnAPcBeytqntXYs4lPAJsHTnfMlwbt+ZYko3AC4AnZjPe2DlOGDcrSXawEOkr\nq+rnM5ptnG7es4GXAV8dvqBdCBxIck1Vza/oZFX1nP4FfBjYMxzvAT7UrL8BuHUtz8vCtxqHgL9a\nhfk2Ag8Dlw5z/Dtw2aI1bwf+Zji+FvjcKn0uT2bWV7Dwbdu21fp/firzLlr/VRaeiF352Vb7kzOD\nT/75w1+qw8A9wHnD9Tngk2PWr3Yo2nmBtwDPAPeP/Lp8hjP+PvCd4S/Y3uHazcA1w/GZwOeBI8C/\nAb++ip/PbtZ7gMdGPo8HVvnP67LzLlo7s1D4Em5JrdPhpx6SJmQoJLUMhaSWoZDUMhSSWoZCUstQ\nSGr9D/8fawRBsjmsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12008ce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'blue'\n",
    "check_color(text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
